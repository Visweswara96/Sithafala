{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9L7rA6f-JW7_"
      },
      "outputs": [],
      "source": [
        "from pdfminer.high_level import extract_text\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains import RetrievalQA\n",
        "from google.colab import files\n",
        "import os\n",
        "import pickle\n",
        "\n",
        "FAISS_STORE_PATH = \"faiss_store.pkl\"\n",
        "\n",
        "def initialize_llm():\n",
        "    return ChatGroq(\n",
        "        temperature=0,\n",
        "        groq_api_key=\"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\",\n",
        "        model_name=\"llama-3.1-70b-versatile\"\n",
        "    )\n",
        "\n",
        "def initialize_embeddings():\n",
        "    try:\n",
        "        from langchain.embeddings import HuggingFaceEmbeddings\n",
        "        return HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "    except ModuleNotFoundError:\n",
        "        from langchain.embeddings import OpenAIEmbeddings\n",
        "        return OpenAIEmbeddings()\n",
        "\n",
        "def extract_text_from_pdfs(uploaded_files):\n",
        "    all_text = \"\"\n",
        "    for file_name in uploaded_files.keys():\n",
        "        all_text += extract_text(file_name) + \"\\n\"\n",
        "    return all_text\n",
        "\n",
        "def build_vector_store(text_data, store_path, embeddings):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
        "    text_chunks = text_splitter.split_text(text_data)\n",
        "    vector_store = FAISS.from_texts(text_chunks, embeddings)\n",
        "    with open(store_path, \"wb\") as f:\n",
        "        pickle.dump(vector_store, f)\n",
        "\n",
        "def load_vector_store(store_path):\n",
        "    with open(store_path, \"rb\") as f:\n",
        "        return pickle.load(f)\n",
        "\n",
        "def run_qa(vector_store, query, llm):\n",
        "    retriever = vector_store.as_retriever()\n",
        "    chain = RetrievalQA.from_llm(llm=llm, retriever=retriever)\n",
        "    return chain.run(query)\n",
        "\n",
        "def main():\n",
        "    uploaded_files = files.upload()\n",
        "    llm = initialize_llm()\n",
        "    embeddings = initialize_embeddings()\n",
        "    if not os.path.exists(FAISS_STORE_PATH):\n",
        "        text_data = extract_text_from_pdfs(uploaded_files)\n",
        "        build_vector_store(text_data, FAISS_STORE_PATH, embeddings)\n",
        "    query = input(\"Ask a question: \").strip()\n",
        "    if query:\n",
        "        vector_store = load_vector_store(FAISS_STORE_PATH)\n",
        "        answer = run_qa(vector_store, query, llm)\n",
        "        print(f\"Answer:\\n{answer}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
